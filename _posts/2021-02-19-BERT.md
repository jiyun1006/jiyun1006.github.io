---
layout: post
title: NLP/BERT
subtitle: "GPT-1, BERT"
categories: AI
tags: [ai]
---

>## GPT-1   
>#### self-attention을 12겹 쌓은 모델   

`encoding시에 masked self attention 사용한다.`   

<img src ="https://user-images.githubusercontent.com/52434993/108461391-fb928100-72bd-11eb-9edf-c69223a332f4.png" width="780px">   

<br><br>


>## BERT(pre-training of Deep Bidirectional Transformers for language understanding)    
>#### language model은 단지 하나의 context만 이용한다.   


- Masked Language Model(MLM)   
  - 각각의 단어를 MASK로 치환하고, 해당 MASK를 맞추도록 모델을 학습시킨다.   


`GPT 모델과 다르게 BERT는 전체 주어진 단어를 attention에 쓴다.(?)`   


 




